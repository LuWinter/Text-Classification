{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc2cbfc8-1ce3-4bf2-8078-5c51dec0281f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jieba\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "477e1fd5-01aa-4094-b8e6-af34c65a5ddc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['时尚', '家居', '教育']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../THUCNews/家居/240053.txt',\n",
       " '../THUCNews/家居/225706.txt',\n",
       " '../THUCNews/家居/244168.txt',\n",
       " '../THUCNews/家居/236354.txt',\n",
       " '../THUCNews/家居/231429.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_file_path(category, frac=0.2):\n",
    "    dir_path = f'THUCNews/{category}/'\n",
    "    file_list = os.listdir(dir_path)\n",
    "    file_list = random.sample(file_list, int(frac * len(file_list)))\n",
    "    file_list = [dir_path + file for file in file_list]\n",
    "    return file_list\n",
    "\n",
    "categories = os.listdir('../THUCNews/')[:3]\n",
    "print(categories)\n",
    "get_file_path(categories[1])[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83ecff74-ef40-48b0-b3b8-9195c8be2dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/97/5s9vc3hs2nj936lfdqch4zsr0000gn/T/jieba.cache\n",
      "Loading model cost 0.400 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['马德兴', '地方官', '为', '全运', '不让', '新星', '留洋', '只', '为', '自己']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_tokens(file):\n",
    "    res = []\n",
    "    with open(file, encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                tokens = list(jieba.cut(line))\n",
    "                tokens = [token for token in tokens if token not in set(\"\\u3000\\n 。,:!！“?…”《》，；—（）-：？^~`[]|()\")]\n",
    "                res += tokens\n",
    "    return res\n",
    "\n",
    "example_file = get_file_path(\"体育\")[0]\n",
    "get_tokens(example_file)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cb148ac-5bc7-41aa-9ea8-bad3ba293880",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2673/2673 [00:05<00:00, 467.21it/s]\n",
      "100%|██████████| 6517/6517 [00:15<00:00, 420.49it/s]\n",
      "100%|██████████| 8387/8387 [00:33<00:00, 252.44it/s]\n"
     ]
    }
   ],
   "source": [
    "topic_list = {}\n",
    "for topic in categories:\n",
    "    file_paths = get_file_path(topic)\n",
    "    topic_list[topic] = [get_tokens(file) for file in tqdm(file_paths)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6663102c-235f-404d-b2ed-898ea6ca9657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196271\n"
     ]
    }
   ],
   "source": [
    "word_to_id, id_to_word = {}, {}\n",
    "for value in topic_list.values():\n",
    "    for text in value:\n",
    "        for word in text:\n",
    "            if word not in word_to_id:\n",
    "                new_id = len(word_to_id)\n",
    "                word_to_id[word] = new_id\n",
    "                id_to_word[new_id] = word\n",
    "\n",
    "n_words = len(word_to_id)\n",
    "print(n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fcc3188-584b-430a-a1c6-ed158e7a41b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6767,    89,   496, 20233,  9530, 40450,  4719,  2110,  5455,  6047,\n",
       "        40451, 40452,     5,   200,  9231,   325,   414,  2202,   414,   754,\n",
       "        13855,     5,  2152,   238,   144,  4719,  3965,    46,  1028,  2152,\n",
       "          118,  1673,   237,   278,   118,    59, 40453,   754,   585,  4807,\n",
       "          249,   247,  6767,  4557,     5, 20233,    66,     5,  4042,   241,\n",
       "         4274,     5,  1163,   348,   840,   241,   118, 12748,     5,  6903,\n",
       "          348,   754,   277,  3600,     5,  2152,   200,   247, 20831,     5,\n",
       "        17694, 40454,  2071, 23879,   927,  2795,   848,  3628,     5, 40455,\n",
       "          225, 21213,  1228,   754,   277, 40456, 17931,     5, 20233,  8467,\n",
       "         4807,   305,   249,   247,  4923,   483,    25,  2897,  1578,  1012,\n",
       "          328, 20233,  6804,  1507, 35025,   247,   148,  3628,     5,    29,\n",
       "          519,     5, 40455,   225,  2795,   848,  5953, 18768,     5, 21213,\n",
       "         1024,  3600,    11,   118,   328,   483,  5953,  3710,     5, 20233,\n",
       "           25, 15570,  4235,  1483, 12634, 25138,  5198,  1801,  2778,  5198,\n",
       "          302, 32047,  1073,  7332,  4927, 40457, 40458,  6912,  2325, 40459,\n",
       "          483, 40460,  1326,  7709,  5198, 26943,  5776, 25138, 32726, 10620,\n",
       "        36081, 40461,  6912,  2325, 40459,   483, 13503,   834,   121,   989,\n",
       "        15570,  1326,  6814,  5198, 26943,  5776, 25138, 32726, 10620, 36081,\n",
       "        40462,  6912,  2325, 40459,  1326, 18956,  5198, 26943,  5776, 25138,\n",
       "        32726, 10620, 36081, 40463,  6912,  2325, 40460,  1326, 10444,  5198,\n",
       "        26943,  5776, 25138, 32726, 10620, 36081,   353,  3377, 17025, 13120,\n",
       "         7560,  3481,  3377, 40464, 17025, 11600,  1157, 40465,    31,  9251,\n",
       "         1392, 22772,  6912, 11600,  6814, 40466,    31, 18956,  7438, 22772,\n",
       "        13120, 11600, 10322, 40467,    31,    60,  4848,   434, 40468, 33002,\n",
       "        40469, 40469, 40470, 40471,  5198,   288,  2777,    89,  5198, 40472,\n",
       "         1017,   238, 40473,   393,   117,   754, 40474, 40475,   754, 17025,\n",
       "         2325,  3582,  1221, 13120,   809, 40470, 40466,  5198,   288,  2777,\n",
       "        12247,  5198, 40472, 17354,   393,   117,   754,  6814, 40475,   754,\n",
       "         6912,  2325,  1503,  1331,   809, 40470, 40466,  5198,   288,  2777,\n",
       "        40476,  5198, 40472,   393,   393,   117,   754,  6814, 40475,   754,\n",
       "         6912,  2325,  1503, 14926,   809, 40470, 40467,  5198,   288,  2777,\n",
       "        40477,  5198, 40472, 17354,   393,   117,   754, 10322, 40475,   754,\n",
       "        13120,  2325,  1503,  3377, 14719,  2485,   682,   519,     5, 40470,\n",
       "           76,   701,  2777,    46, 40478,  1080,    11,  8184,    59, 40478,\n",
       "        28097,  2071, 40479,  8184, 17720, 40480,   355,   519,     5, 40481,\n",
       "        40482,  4592, 40483, 40484,    76,   170,  2777, 40485, 21231,  5198,\n",
       "          701,  2777,     5, 24150,   241,  2575,  2690,   170,  2777,     5,\n",
       "        24150,  1369,   183, 40486,  2631,   483, 40487,  2605,  1687,   170,\n",
       "         2777,     5,  8261, 40470,     5, 24150,   241,  2575,  2690, 40484,\n",
       "            5, 24150,  1369,   241,   690, 40488,   483,  3682,     5,  3914,\n",
       "         1687, 40484,     5,  3914,  3600,  5301,   754, 40489,     5,  1495,\n",
       "           25,   907, 26957,   241, 40465,  4848,   211,   241,  2796,   268,\n",
       "        26957,   769,   386, 20233,  2110,  5455,     5, 40450,   663,    39,\n",
       "         2086,     5,  2076,  3722, 40490,  6915,   754,     5,  6058,  3692,\n",
       "           81,   143,    59, 40454, 40430,  2071,   754,     5, 18100,    25,\n",
       "          927,  2228,     5, 11094, 20233,  5525,   118,  4687,     5,   149,\n",
       "        40454,   934,   551,  2896, 25786, 20233,     5,  3592,    13,  4237,\n",
       "         1782,    59, 23879,   837,   305,  1606,   203,  1507, 27914,  1425,\n",
       "         1163, 20233,   439,  1425,   278,   203,     5, 20233,    59,   907,\n",
       "          709, 10572, 15192,  1888,     5,  4109,  6903,   348,   108,   393,\n",
       "        25850,     5, 32934,   348,   131,    46,  1620, 12748,   348,   560,\n",
       "        12748, 40491,   137,  1163, 20233,   461,   368,   393, 20233,     5,\n",
       "        38404, 13763,   105,   453,   455, 26571, 40492,   560,  1507, 27914,\n",
       "            5,  5826,  4442,   355, 20233,   229,  9121, 40454, 11230,  9231,\n",
       "            5,   111,   453,  2817,   241, 40493,  1425,     5,  4274,  8164,\n",
       "           96,  5915,   754,  1359,   794,  1583,   117,   121,  9868,   200,\n",
       "          247, 16135,  6047,     5,    87,  9102,   117,    46, 20831,     5,\n",
       "         2050,   769])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_to_tensor(text):\n",
    "    tensor = torch.zeros(len(text), dtype=torch.long)\n",
    "    for li, word in enumerate(text):\n",
    "        try:\n",
    "            ind = word_to_id[word]\n",
    "        except KeyError:\n",
    "            ind = n_words - 1\n",
    "        tensor[li] = ind\n",
    "    return tensor\n",
    "\n",
    "example_file = get_file_path(\"时尚\")[0]\n",
    "text_to_tensor(get_tokens(example_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bc7f827-6b31-4a10-9083-fa811c5bd196",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2673/2673 [00:01<00:00, 1754.63it/s]\n",
      "100%|██████████| 6517/6517 [00:04<00:00, 1545.03it/s]\n",
      "100%|██████████| 8387/8387 [00:09<00:00, 926.74it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: 14061\n",
      "Test data size: 3516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_data = []\n",
    "    \n",
    "for ind, value in enumerate(topic_list.values()):\n",
    "    for tokens in tqdm(value):\n",
    "        all_data.append((text_to_tensor(tokens), torch.tensor([ind], dtype=torch.long)))\n",
    "    \n",
    "random.shuffle(all_data)\n",
    "data_len = len(all_data)\n",
    "split_ratio = 0.8\n",
    "\n",
    "train_data = all_data[:int(data_len * split_ratio)]\n",
    "test_data = all_data[int(data_len * split_ratio):]\n",
    "print(\"Train data size:\", len(train_data))\n",
    "print(\"Test data size:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb8c4c8-605d-447d-81b0-bd28a93bb686",
   "metadata": {},
   "source": [
    "## 2. 构造数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90091494-7f9d-4f78-8cec-3dbc5675ee79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, word_count, embedding_size, hidden_size, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(word_count, embedding_size)\n",
    "        self.LSTM = nn.LSTM(embedding_size, hidden_size, num_layers=2, bidirectional=True, batch_first=True)\n",
    "        self.cls = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=0)\n",
    "        \n",
    "    def forward(self, input_tensor):\n",
    "        word_vector = self.embedding(input_tensor)\n",
    "        output = self.LSTM(word_vector)[0][0][len(input_tensor)-1]\n",
    "        output = output.reshape(2, -1).sum(axis=0)\n",
    "        output = self.cls(output)\n",
    "        output = self.softmax(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed6f0d2c-e232-43e5-b219-324c9091f33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lstm(rnn, input_tensor):\n",
    "    output = rnn(input_tensor.unsqueeze(dim=0))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "075ac3a4-3a7d-4548-9811-788f9b3515d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(rnn, criterion, input_tensor, category_tensor):\n",
    "    rnn.zero_grad()\n",
    "    output = run_lstm(rnn, input_tensor)\n",
    "    loss = criterion(output.unsqueeze(dim=0), category_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    # 根据梯度更新模型的参数\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "\n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "236469a5-4bfa-4512-8588-ec0da2a3dbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(lstm, input_tensor):\n",
    "    with torch.no_grad():\n",
    "        output = run_lstm(lstm, input_tensor)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45b0be63-a8b1-4c6b-b4a4-3b91b1cdb856",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14061/14061 [4:19:42<00:00,  1.11s/it]     \n",
      "100%|██████████| 3516/3516 [34:57<00:00,  1.68it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8734357224118316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14061/14061 [2:58:24<00:00,  1.31it/s]    \n",
      "100%|██████████| 3516/3516 [01:38<00:00, 35.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.9169510807736063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14061/14061 [46:48<00:00,  5.01it/s] \n",
      "100%|██████████| 3516/3516 [01:38<00:00, 35.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.9416951080773607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14061/14061 [47:11<00:00,  4.97it/s] \n",
      "100%|██████████| 3516/3516 [01:39<00:00, 35.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.9434015927189988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14061/14061 [47:03<00:00,  4.98it/s] \n",
      "100%|██████████| 3516/3516 [01:38<00:00, 35.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.9527872582480091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14061/14061 [46:54<00:00,  5.00it/s] \n",
      "100%|██████████| 3516/3516 [01:38<00:00, 35.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.947098976109215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14061/14061 [47:09<00:00,  4.97it/s] \n",
      "100%|██████████| 3516/3516 [01:38<00:00, 35.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.9496587030716723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14061/14061 [47:25<00:00,  4.94it/s] \n",
      "100%|██████████| 3516/3516 [01:39<00:00, 35.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.9553469852104665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14061/14061 [47:22<00:00,  4.95it/s] \n",
      "100%|██████████| 3516/3516 [01:39<00:00, 35.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.9573378839590444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14061/14061 [4:30:59<00:00,  1.16s/it]     \n",
      "100%|██████████| 3516/3516 [01:38<00:00, 35.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.9556313993174061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "epoch = 10\n",
    "embedding_size = 128\n",
    "n_hidden = 64\n",
    "n_categories = 3\n",
    "learning_rate = 0.005\n",
    "lstm = LSTM(n_words, embedding_size, n_hidden, n_categories)\n",
    "criterion = nn.NLLLoss()\n",
    "loss_sum = 0\n",
    "all_losses = []\n",
    "plot_every = 100\n",
    "for e in range(epoch):\n",
    "    for ind, (title_tensor, label) in enumerate(tqdm(train_data)):\n",
    "        output, loss = train(lstm, criterion, title_tensor, label)\n",
    "        loss_sum += loss\n",
    "        if ind % plot_every == 0:\n",
    "            all_losses.append(loss_sum / plot_every)\n",
    "            loss_sum = 0\n",
    "    c = 0\n",
    "    for title, category in tqdm(test_data):\n",
    "        output = evaluate(lstm, title)\n",
    "        topn, topi = output.topk(1)\n",
    "        if topi.item() == category[0].item():\n",
    "            c += 1\n",
    "    print('accuracy', c / len(test_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
