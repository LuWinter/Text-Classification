{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a31b3aa-a6c8-44c2-ba6e-0c495a98f673",
   "metadata": {},
   "source": [
    "## 0. 参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51ae666e-0b3b-4327-9680-d6bf490c449e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training parameters\n",
    "max_train_epochs = 5\n",
    "warmup_proportion = 0.1\n",
    "gradient_accumulation_steps = 4\n",
    "batch_size = 8\n",
    "learning_rate = 2e-5\n",
    "weight_decay = 0.01\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "## Dataset parameters\n",
    "training_set_split = 0.7\n",
    "label_to_id = {'时尚': 0, '家居': 1, '教育': 2}\n",
    "sample_ratio = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ed80bc-5015-480e-ab51-4d00b1f7d2c7",
   "metadata": {},
   "source": [
    "## 1. 读取数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b72ed6a-4831-41b5-9450-3c0f1039dade",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-13 15:03:13.522021: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-13 15:03:14.712347: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-13 15:03:14.712457: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-13 15:03:14.712471: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn, LongTensor\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import BertTokenizer, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e23d471f-5769-48e9-9acf-8041e48e8ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_label(categories, frac):\n",
    "    _files, _labels = [], []\n",
    "    for category in categories:\n",
    "        dir_path = f'/THUCNews/{category}/'\n",
    "        file_list = os.listdir(dir_path)\n",
    "        file_list = random.sample(file_list, int(frac * len(file_list)))\n",
    "        file_list = [dir_path + file for file in file_list]\n",
    "        _files += file_list\n",
    "        _labels += [category] * len(file_list)\n",
    "    return _files, _labels\n",
    "\n",
    "all_files, all_labels = get_file_label(categories=list(label_to_id.keys()), frac=sample_ratio)\n",
    "dataset_length = len(all_labels)\n",
    "data_list = [(all_files[idx], all_labels[idx]) for idx in range(dataset_length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ff3cbe3-ef5a-4b94-a6fb-a270621b4428",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdata.datapipes.map import SequenceWrapper\n",
    "\n",
    "datapipe = SequenceWrapper(data_list).shuffle()\n",
    "train_datapipe, test_datapipe = datapipe.random_split(\n",
    "    total_length=dataset_length, \n",
    "    weights={\"train\": training_set_split, \"test\": 1-training_set_split}, \n",
    "    seed=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "316b1835-700e-4861-9a05-005c1de942a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7607, 2238, 4384, 3952, 6381, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "tokenizer(\"飞屋环游记\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb379787-77ce-4adb-bae2-f4dc5047cff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(file):\n",
    "    text = []\n",
    "    with open(file, encoding='utf8') as f:\n",
    "        [text.append(line.strip()) for line in f if line.strip()]\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def collate_batch(batch):    \n",
    "    r = tokenizer([get_text(b[0]) for b in batch], padding='max_length', max_length=512, truncation=True)\n",
    "    input_ids = LongTensor(r['input_ids'])\n",
    "    attention_mask = LongTensor(r['attention_mask'])\n",
    "    label = LongTensor([label_to_id[b[1]] for b in batch])\n",
    "\n",
    "    return input_ids, attention_mask, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8dc9386-fb40-45e7-b945-1d2afe070b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 101, 2990, 1184,  ..., 2349, 7770,  102],\n",
       "         [ 101, 8166, 2399,  ...,  702, 6121,  102],\n",
       "         [ 101, 3173, 3857,  ..., 3698, 1962,  102],\n",
       "         ...,\n",
       "         [ 101, 5299, 1745,  ...,    0,    0,    0],\n",
       "         [ 101,  860, 7741,  ..., 6574, 7030,  102],\n",
       "         [ 101, 7305, 4970,  ...,  679,  788,  102]]),\n",
       " tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]]),\n",
       " tensor([2, 1, 1, 2, 2, 0, 2, 1]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = DataLoader(train_datapipe, batch_size=batch_size, collate_fn=collate_batch)\n",
    "test_loader = DataLoader(test_datapipe, batch_size=batch_size, collate_fn=collate_batch)\n",
    "\n",
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c357be29-354a-49eb-85ef-ba6ddc7ec103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score():\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for step, batch in enumerate(tqdm(test_loader)):\n",
    "        model.eval()            # turn to Evaluation Mode\n",
    "        with torch.no_grad():\n",
    "            input_ids, attention_mask = (b.to(device) for b in batch[:2])\n",
    "        y_true += batch[2].numpy().tolist()\n",
    "        logist = model(input_ids, attention_mask)[0]\n",
    "        result = torch.argmax(logist, 1).cpu().numpy().tolist()\n",
    "        y_pred += result\n",
    "    correct = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] == y_pred[i]:\n",
    "            correct += 1\n",
    "    accuracy = correct / len(y_pred)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee3d9fb9-c457-4d41-97f4-84ee49303d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03081ada-2bb4-4cec-9a1a-b00982f4a147",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.27.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "Training(total) Steps: 23066\n",
      "Warm-up Steps: 2306\n"
     ]
    }
   ],
   "source": [
    "## Set model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-chinese', num_labels=len(label_to_id))\n",
    "model.to(device)\n",
    "print(model.config)\n",
    "\n",
    "## Optimizer settings\n",
    "no_decay = ['bias', 'LayerNorm.weight'] # No decay for bias and LayerNorm\n",
    "param_optimizer = list(model.named_parameters())\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "# print(\"Parameter Names:\", [name for name, _ in param_optimizer if not any(nd in name for nd in no_decay)])\n",
    "\n",
    "## Scheduler settings\n",
    "total_steps = int(dataset_length * training_set_split) // gradient_accumulation_steps * max_train_epochs + 1\n",
    "warmup_steps = int(warmup_proportion * total_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "print(f'Training(total) Steps: {total_steps}\\nWarm-up Steps: {warmup_steps}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ebcca6-10ea-4076-97b6-c77976fbf0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2307it [19:15,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1 Epoch Mean Loss 0.0196 Time 19.26 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "989it [03:12,  5.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9922882427307206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2307it [19:10,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 2 Epoch Mean Loss 0.0031 Time 19.17 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "989it [03:11,  5.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9932996207332491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2307it [19:13,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 3 Epoch Mean Loss 0.0016 Time 19.22 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "989it [03:10,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9973451327433628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2307it [19:05,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 4 Epoch Mean Loss 0.0030 Time 19.10 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "989it [03:09,  5.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9987357774968394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2307it [19:09,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 5 Epoch Mean Loss 0.0014 Time 19.16 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "599it [01:55,  5.27it/s]"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_train_epochs):\n",
    "    b_time = time.time()\n",
    "    model.train()                 # turn to Training Mode\n",
    "    for step, batch in enumerate(tqdm(train_loader)):\n",
    "        input_ids, attention_mask, label = [b.to(device) for b in batch]\n",
    "        loss = model(input_ids, attention_mask, labels=label)\n",
    "        loss = loss[0]\n",
    "        loss.backward()\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step() \n",
    "            optimizer.zero_grad()\n",
    "    print('Epoch = %d Epoch Mean Loss %.4f Time %.2f min' % (epoch+1, loss.item(), (time.time() - b_time)/60))\n",
    "    print(get_score())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
