{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "121bbb6c-eb33-44fb-b8b2-7b9c28e118f2",
   "metadata": {},
   "source": [
    "## 12.7.1 载入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8aad98d-b321-4288-94fd-5cbcb9d65872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['北师教育学，你我一起努力，让胜利酣畅淋漓。', '考博英语词汇', '出售人大新闻学院2015年考研权威资料', '【脑科院 郭桃梅课题组】科研助理招聘', '管理学院的同学帮帮忙呐～']\n",
      "['【字节跳动内推】校招岗位全面开放，帮查进度！', '招聘兼职/ 笔试考务 /200-300 每人', '国企出版社招聘坐班兼职生', '【在线早教】教研实习生招聘', '【兼职】心理学公众号寻兼职写手']\n"
     ]
    }
   ],
   "source": [
    "# 数据导入到列表\n",
    "academy_titles, job_titles = [], []\n",
    "\n",
    "with open(r'academy_titles.txt', encoding='utf8') as f:\n",
    "    for l in f:\n",
    "        academy_titles.append(l.strip())\n",
    "\n",
    "with open(r'job_titles.txt', encoding='utf8') as f:\n",
    "    for l in f:\n",
    "        job_titles.append(l.strip())\n",
    "\n",
    "print(academy_titles[:5])\n",
    "print(job_titles[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66755731-4929-43ad-9422-e1e94718b7e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T10:12:19.977971Z",
     "start_time": "2021-01-07T10:12:19.969237Z"
    }
   },
   "outputs": [],
   "source": [
    "data_list = []\n",
    "\n",
    "for title in academy_titles:\n",
    "    data_list.append((title, 0))\n",
    "\n",
    "for title in job_titles:\n",
    "    data_list.append((title, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62fdb4e6-6ab6-4de4-8185-68886bf39d02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T10:12:20.844124Z",
     "start_time": "2021-01-07T10:12:20.834090Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of titles: 77\n"
     ]
    }
   ],
   "source": [
    "max_length = 0\n",
    "for case in data_list:\n",
    "    max_length = max(max_length, len(case[0])+2)\n",
    "print(f\"Max length of titles: {max_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb50638d-3fed-407b-889e-3efba98d444d",
   "metadata": {},
   "source": [
    "## 12.7.2 导入包和设置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dbd9288-48a5-4a15-92b6-d1b948e148a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "from torch import nn, LongTensor\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import BertTokenizer, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c80a629-8be2-4f7c-beda-4e2429bad447",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6247a79-5b88-4391-9291-c2373760bff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training parameters\n",
    "max_train_epochs = 5\n",
    "warmup_proportion = 0.05\n",
    "gradient_accumulation_steps = 4\n",
    "batch_size = 8\n",
    "learning_rate = 2e-5\n",
    "weight_decay = 0.01\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "## Dataset parameters\n",
    "training_set_split = 0.7\n",
    "dataset_length = len(data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c208781-792d-4ec9-a7d5-eca63cfab1a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 12.7.3 定义DataPipe和DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5f4e8f1-3170-4763-a951-753bf10e6825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7607, 2238, 4384, 3952, 6381, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n",
    "def text_to_token(text):\n",
    "    return tokenizer.encode_plus(text, max_length=max_length, padding=\"max_length\")\n",
    "\n",
    "text_to_token(\"飞屋环游记\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa7edf18-eded-48fd-b1ec-4bc1cb45ef6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdata.datapipes.map import SequenceWrapper\n",
    "\n",
    "datapipe = SequenceWrapper(data_list).shuffle()\n",
    "train_datapipe, test_datapipe = datapipe.random_split(\n",
    "    total_length=dataset_length, \n",
    "    weights={\"train\": training_set_split, \"test\": 1-training_set_split}, \n",
    "    seed=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37cd6164-cab3-4553-a68e-037db9f3612e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('【脑科院 郭桃梅课题组】科研助理招聘', 0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_datapipe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba7fa76e-e181-4a56-8429-ebdf3e4cee31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    r = tokenizer([b[0] for b in batch], padding=True)\n",
    "    input_ids = LongTensor(r['input_ids'])\n",
    "    attention_mask = LongTensor(r['attention_mask'])\n",
    "    label = LongTensor([b[1] for b in batch])\n",
    "    return input_ids, attention_mask, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02efdc6d-6764-47c7-a869-945ab2f0c208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_datapipe, batch_size=batch_size, collate_fn=collate_batch)\n",
    "test_loader = DataLoader(test_datapipe, batch_size=batch_size, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b836a985-fdd3-471b-b1d5-93cdd4c74f56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  101,  4343,  1767,  2875,   772,  1501,  6817,  5852,  8020,  6440,\n",
       "           4923,  1403,  8021,  2141,   739,  4495,  1568,  8013,   102,     0,\n",
       "              0,     0,     0],\n",
       "         [  101,  2875,  5470,  8038,  2825,  3318,  3118,  2898,   113,  7270,\n",
       "           3309,  3300,  3126,  8021,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0],\n",
       "         [  101,  8127,  5440,  4777,  1325,  1380,  2110,  7368,  4638,  2110,\n",
       "           7270,  2110,  1995,   812,  1762,  1525,  7027,   102,     0,     0,\n",
       "              0,     0,     0],\n",
       "         [  101,  1266,  1920,  7032,  6084,   683,  4798,  7444,  6206,  5440,\n",
       "           3124,  3780,   720,   102,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0],\n",
       "         [  101,  1160,  2365,  8024,  2347,  2875,  1168,   102,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0],\n",
       "         [  101,  8232,  2399,   809,  1184,  1266,  2360,  1920,  3136,  5509,\n",
       "           2110,  5440,  4777,  4696,  7579,  2130,  3146,  4276,   117,   102,\n",
       "              0,     0,     0],\n",
       "         [  101,  2875,   671,   702,   782,  5356,  1091, 11685, 11836,  8794,\n",
       "           2552,  4415,  2110,  2141,  7741,  4923,  2415,  8024,  2845,  6992,\n",
       "            794,   831,   102],\n",
       "         [  101,  1139,  8467,  8158,  1265,  2110,  3136,  5509,  6598,  3160,\n",
       "           1350, 10745,  3136,  5509,  5341,  1394,  6598,  3160,   102,     0,\n",
       "              0,     0,     0]]),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]]),\n",
       " tensor([1, 1, 0, 0, 1, 0, 1, 0]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86f0693-c7c2-4a83-90d2-224005f12995",
   "metadata": {},
   "source": [
    "## 12.7.4 定义评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3da49ad7-4206-486e-b15e-7ab35ebe5546",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T10:12:27.578031Z",
     "start_time": "2021-01-07T10:12:27.567923Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_score():\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for step, batch in enumerate(tqdm(test_loader)):\n",
    "        model.eval()            # turn to Evaluation Mode\n",
    "        with torch.no_grad():\n",
    "            input_ids, attention_mask = (b.to(device) for b in batch[:2])\n",
    "        y_true += batch[2].numpy().tolist()\n",
    "        logist = model(input_ids, attention_mask)[0]\n",
    "        result = torch.argmax(logist, 1).cpu().numpy().tolist()\n",
    "        y_pred += result\n",
    "    correct = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] == y_pred[i]:\n",
    "            correct += 1\n",
    "    accuracy = correct / len(y_pred)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa26b69-1391-47c3-addb-4bc3544323f9",
   "metadata": {},
   "source": [
    "## 12.7.5 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ec81fe8-04f7-4a56-b872-dddb7c313697",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T10:12:43.180553Z",
     "start_time": "2021-01-07T10:12:27.580304Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter Names: ['bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.output.dense.weight', 'bert.pooler.dense.weight', 'classifier.weight']\n",
      "Training(total) Steps: 6216\n",
      "Warm-up Steps: 310\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-chinese')\n",
    "model.to(device)\n",
    "\n",
    "## Optimizer settings\n",
    "no_decay = ['bias', 'LayerNorm.weight'] # No decay for bias and LayerNorm\n",
    "param_optimizer = list(model.named_parameters())\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "print(\"Parameter Names:\", [name for name, _ in param_optimizer if not any(nd in name for nd in no_decay)])\n",
    "\n",
    "## Scheduler settings\n",
    "total_steps = int(dataset_length * training_set_split) // gradient_accumulation_steps * max_train_epochs + 1\n",
    "warmup_steps = int(warmup_proportion * total_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "print(f'Training(total) Steps: {total_steps}\\nWarm-up Steps: {warmup_steps}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4378bb11-1680-4e7e-9d81-988b8c604479",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T10:17:26.475056Z",
     "start_time": "2021-01-07T10:12:48.170852Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "622it [04:43,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1 Epoch Mean Loss 0.0107 Time 4.73 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "267it [00:36,  7.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9990623534927332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "622it [04:55,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 2 Epoch Mean Loss 0.0012 Time 4.92 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "267it [00:36,  7.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "622it [04:52,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 3 Epoch Mean Loss 0.0014 Time 4.88 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "267it [00:35,  7.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "622it [04:52,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 4 Epoch Mean Loss 0.0002 Time 4.88 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "267it [00:35,  7.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "622it [04:52,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 5 Epoch Mean Loss 0.0009 Time 4.87 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "267it [00:38,  6.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9990623534927332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_train_epochs):\n",
    "    b_time = time.time()\n",
    "    model.train()                 # turn to Training Mode\n",
    "    for step, batch in enumerate(tqdm(train_loader)):\n",
    "        input_ids, attention_mask, label = [b.to(device) for b in batch]\n",
    "        loss = model(input_ids, attention_mask, labels=label)\n",
    "        loss = loss[0]\n",
    "        loss.backward()\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step() \n",
    "            optimizer.zero_grad()\n",
    "    print('Epoch = %d Epoch Mean Loss %.4f Time %.2f min' % (epoch+1, loss.item(), (time.time() - b_time)/60))\n",
    "    print(get_score())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
